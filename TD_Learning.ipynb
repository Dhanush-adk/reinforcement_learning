{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhanush-adk/reinforcement_learning/blob/main/TD_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "hHiNJeKm5XxO"
      },
      "outputs": [],
      "source": [
        "class MoverEnvironment:\n",
        "    def __init__(self, size=7, eater_actions=None, eater_policy=None):\n",
        "        self.size = size\n",
        "        self.true_goal = (6, 6)  # True goal at (6, 6)\n",
        "        self.fake_goal = (1, 5)  # Fake goal at (1, 5)\n",
        "        self.initial_resources = 100  # High initial resource value for both goals\n",
        "        self.resources = {self.true_goal: self.initial_resources, self.fake_goal: self.initial_resources}\n",
        "        self.eater_actions = eater_actions or [(1, 0), (0, 1), (0.5, 0.5), (0.2, 0.8), (0.6, 0.4),\n",
        "                                               (0.8, 0.2), (0.4, 0.6), (0.7, 0.3), (0.3, 0.7)]\n",
        "        self.eater_policy = eater_policy or {}\n",
        "        self.moves = ['Up', 'Down', 'Left', 'Right']\n",
        "        self.visited_fake_goal = False\n",
        "        self.state = (0, 0)\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (0, 0)\n",
        "        self.resources = {self.true_goal: self.initial_resources, self.fake_goal: self.initial_resources}\n",
        "        self.visited_fake_goal = False\n",
        "        return self.state\n",
        "\n",
        "    def step(self, curr_state, action):\n",
        "        move_offsets = [(0, 1),(0, -1),(-1, 0), (1, 0)]\n",
        "        new_state = (curr_state[0] + move_offsets[action][0], curr_state[1] + move_offsets[action][1])\n",
        "\n",
        "        # Ensure the agent doesn't move out of bounds\n",
        "        new_state = (\n",
        "            max(0, min(new_state[0], self.size - 1)),\n",
        "            max(0, min(new_state[1], self.size - 1))\n",
        "        )\n",
        "\n",
        "        # Calculate the linear index for the eater policy\n",
        "        index = new_state[1] * self.size + new_state[0]\n",
        "\n",
        "        # Apply the eater action based on the eater policy or use a default action\n",
        "        eater_index = self.eater_policy.get((index, action), 0)\n",
        "        consumption = self.eater_actions[eater_index]\n",
        "        self.resources[self.true_goal] -= consumption[1]\n",
        "        self.resources[self.fake_goal] -= consumption[0]\n",
        "\n",
        "        # Update state\n",
        "        self.state = new_state\n",
        "        reward = -1  # Default step cost\n",
        "        deceive = (2,5)\n",
        "        # Logic for fake goal interaction\n",
        "        if self.state == deceive and not self.visited_fake_goal:\n",
        "            self.visited_fake_goal = True\n",
        "            reward += 50  # Reward for first visiting the fake goal\n",
        "        elif self.state == deceive:\n",
        "            reward += 40  # Reward for revisiting the fake goal\n",
        "\n",
        "        # Logic for true goal interaction\n",
        "        if self.state == self.true_goal:\n",
        "            if not self.visited_fake_goal:\n",
        "                reward -= 100  # Penalty for reaching true goal before fake goal\n",
        "            else:\n",
        "                reward += 815  # Big reward for reaching true goal after fake\n",
        "\n",
        "        # Check for resource depletion\n",
        "        if self.resources[self.true_goal] <= 0 or self.resources[self.fake_goal] <= 0:\n",
        "            reward -= 100  # Severe penalty for running out of resources\n",
        "\n",
        "        # Determine if the game is over (reaching the true goal)\n",
        "        done = self.state == self.true_goal\n",
        "\n",
        "        return self.state, reward, done\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = MoverEnvironment(\n",
        "    eater_policy=\n",
        "{(1, 0): 7, (1, 1): 7, (1, 2): 7, (1, 3): 7, (2, 0): 7, (2, 1): 7, (2, 2): 7, (2, 3): 7, (3, 0): 7, (3, 1): 7, (3, 2): 7, (3, 3): 7, (4, 0): 7, (4, 1): 7, (4, 2): 7, (4, 3): 7, (5, 0): 7, (5, 1): 7, (5, 2): 7, (5, 3): 7, (6, 0): 7, (6, 1): 7, (6, 2): 7, (6, 3): 7, (7, 0): 7, (7, 1): 7, (7, 2): 7, (7, 3): 7, (8, 0): 7, (8, 1): 7, (8, 2): 7, (8, 3): 7, (9, 0): 7, (9, 1): 7, (9, 2): 7, (9, 3): 7, (10, 0): 7, (10, 1): 7, (10, 2): 7, (10, 3): 7, (11, 0): 7, (11, 1): 7, (11, 2): 7, (11, 3): 7, (12, 0): 5, (12, 1): 5, (12, 2): 5, (12, 3): 5, (13, 0): 5, (13, 1): 5, (13, 2): 5, (13, 3): 5, (14, 0): 5, (14, 1): 5, (14, 2): 5, (14, 3): 5, (15, 0): 5, (15, 1): 5, (15, 2): 5, (15, 3): 5, (16, 0): 5, (16, 1): 5, (16, 2): 5, (16, 3): 5, (17, 0): 5, (17, 1): 5, (17, 2): 5, (17, 3): 5, (18, 0): 5, (18, 1): 5, (18, 2): 5, (18, 3): 5, (19, 0): 5, (19, 1): 5, (19, 2): 5, (19, 3): 5, (20, 0): 5, (20, 1): 5, (20, 2): 5, (20, 3): 5, (21, 0): 5, (21, 1): 5, (21, 2): 5, (21, 3): 5, (22, 0): 5, (22, 1): 5, (22, 2): 5, (22, 3): 5, (23, 0): 5, (23, 1): 5, (23, 2): 5, (23, 3): 5, (24, 0): 5, (24, 1): 5, (24, 2): 5, (24, 3): 5, (25, 0): 5, (25, 1): 5, (25, 2): 5, (25, 3): 5, (26, 0): 5, (26, 1): 5, (26, 2): 5, (26, 3): 5, (27, 0): 5, (27, 1): 5, (27, 2): 5, (27, 3): 5, (28, 0): 5, (28, 1): 5, (28, 2): 5, (28, 3): 5, (29, 0): 5, (29, 1): 5, (29, 2): 5, (29, 3): 5, (30, 0): 5, (30, 1): 5, (30, 2): 5, (30, 3): 5, (31, 0): 5, (31, 1): 5, (31, 2): 5, (31, 3): 5, (32, 0): 0, (32, 1): 5, (32, 2): 5, (32, 3): 5, (33, 0): 0, (33, 1): 5, (33, 2): 5, (33, 3): 5, (34, 0): 0, (34, 1): 5, (34, 2): 5, (34, 3): 5, (35, 0): 0, (35, 1): 5, (35, 2): 5, (35, 3): 5, (36, 0): 0, (36, 1): 5, (36, 2): 5, (36, 3): 5, (37, 0): 0, (37, 1): 5, (37, 2): 5, (37, 3): 5, (38, 0): 0, (38, 1): 5, (38, 2): 5, (38, 3): 5, (39, 0): 0, (39, 1): 5, (39, 2): 5, (39, 3): 8, (40, 0): 5, (40, 1): 5, (40, 2): 8, (40, 3): 8, (41, 0): 5, (41, 1): 4, (41, 2): 8, (41, 3): 8, (42, 0): 2, (42, 1): 8, (42, 2): 8, (42, 3): 8, (43, 0): 8, (43, 1): 8, (43, 2): 8, (43, 3): 8, (44, 0): 8, (44, 1): 8, (44, 2): 8, (44, 3): 8, (45, 0): 8, (45, 1): 8, (45, 2): 8, (45, 3): 8, (46, 0): 8, (46, 1): 8, (46, 2): 8, (46, 3): 8, (47, 0): 8, (47, 1): 8, (47, 2): 8, (47, 3): 8, (48, 0): 8, (48, 1): 8, (48, 2): 8, (48, 3): 3, (49, 0): 8, (49, 1): 8, (49, 2): 3, (49, 3): 1})"
      ],
      "metadata": {
        "id": "XI0yIdWXBZ3r"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def td_learning(env, episodes, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
        "    # State space is the product of x and y dimensions of the grid\n",
        "    # Action space size is 4 (Up, Down, Left, Right)\n",
        "    q_values = np.zeros((env.size, env.size, len(env.moves)))\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        current_state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.randint(0, 3)\n",
        "            else:\n",
        "                action = np.argmax(q_values[current_state[0], current_state[1], :])\n",
        "\n",
        "            # Execute action\n",
        "            next_state, reward, done = env.step(current_state, action)\n",
        "\n",
        "            # TD(0) Update\n",
        "            best_next_action = np.argmax(q_values[next_state[0], next_state[1], :])\n",
        "            td_target = reward + gamma * q_values[next_state[0], next_state[1], best_next_action]\n",
        "            td_error = td_target - q_values[current_state[0], current_state[1], action]\n",
        "            q_values[current_state[0], current_state[1], action] += alpha * td_error\n",
        "\n",
        "            # Move to the next state\n",
        "            current_state = next_state\n",
        "\n",
        "    return q_values\n"
      ],
      "metadata": {
        "id": "xH9dhwcB838n"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def derive_policy_from_q_values(q_values):\n",
        "    policy = {}\n",
        "    size = q_values.shape[0]  # Assuming q_values has shape (size, size, number of actions)\n",
        "    for i in range(size):\n",
        "        for j in range(size):\n",
        "            # Select the action with the maximum Q-value at state (i, j)\n",
        "            best_action_index = np.argmax(q_values[i, j, :])\n",
        "            # Map to policy dictionary\n",
        "            policy[(i, j)] = best_action_index\n",
        "    return policy\n"
      ],
      "metadata": {
        "id": "HU69AtPD9EfZ"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_values = td_learning(env, 10000)  # example of obtaining q_values\n",
        "\n",
        "# Derive the policy from Q-values\n",
        "policy = derive_policy_from_q_values(q_values)\n",
        "\n",
        "# Optionally, convert action indexes to action names if helpful\n",
        "action_names = ['Up', 'Down', 'Left', 'Right']\n",
        "named_policy = {state: action_names[action] for state, action in policy.items()}\n",
        "\n",
        "print(named_policy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYZYYevo-CH_",
        "outputId": "a7db3721-8772-4482-ce88-f1a1c26b5f94"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(0, 0): 'Right', (0, 1): 'Down', (0, 2): 'Down', (0, 3): 'Right', (0, 4): 'Right', (0, 5): 'Right', (0, 6): 'Right', (1, 0): 'Up', (1, 1): 'Right', (1, 2): 'Down', (1, 3): 'Down', (1, 4): 'Right', (1, 5): 'Down', (1, 6): 'Down', (2, 0): 'Up', (2, 1): 'Up', (2, 2): 'Up', (2, 3): 'Up', (2, 4): 'Up', (2, 5): 'Up', (2, 6): 'Right', (3, 0): 'Left', (3, 1): 'Left', (3, 2): 'Left', (3, 3): 'Left', (3, 4): 'Left', (3, 5): 'Right', (3, 6): 'Right', (4, 0): 'Left', (4, 1): 'Up', (4, 2): 'Left', (4, 3): 'Down', (4, 4): 'Left', (4, 5): 'Right', (4, 6): 'Right', (5, 0): 'Left', (5, 1): 'Left', (5, 2): 'Left', (5, 3): 'Up', (5, 4): 'Up', (5, 5): 'Up', (5, 6): 'Right', (6, 0): 'Up', (6, 1): 'Up', (6, 2): 'Left', (6, 3): 'Up', (6, 4): 'Up', (6, 5): 'Left', (6, 6): 'Up'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = MoverEnvironment(\n",
        "    eater_policy=\n",
        "{(1, 0): 7, (1, 1): 7, (1, 2): 7, (1, 3): 7, (2, 0): 7, (2, 1): 7, (2, 2): 7, (2, 3): 7, (3, 0): 7, (3, 1): 7, (3, 2): 7, (3, 3): 7, (4, 0): 7, (4, 1): 7, (4, 2): 7, (4, 3): 7, (5, 0): 7, (5, 1): 7, (5, 2): 7, (5, 3): 7, (6, 0): 7, (6, 1): 7, (6, 2): 7, (6, 3): 7, (7, 0): 7, (7, 1): 7, (7, 2): 7, (7, 3): 7, (8, 0): 7, (8, 1): 7, (8, 2): 7, (8, 3): 7, (9, 0): 7, (9, 1): 7, (9, 2): 7, (9, 3): 7, (10, 0): 7, (10, 1): 7, (10, 2): 7, (10, 3): 7, (11, 0): 7, (11, 1): 7, (11, 2): 7, (11, 3): 7, (12, 0): 5, (12, 1): 5, (12, 2): 5, (12, 3): 5, (13, 0): 5, (13, 1): 5, (13, 2): 5, (13, 3): 5, (14, 0): 5, (14, 1): 5, (14, 2): 5, (14, 3): 5, (15, 0): 5, (15, 1): 5, (15, 2): 5, (15, 3): 5, (16, 0): 5, (16, 1): 5, (16, 2): 5, (16, 3): 5, (17, 0): 5, (17, 1): 5, (17, 2): 5, (17, 3): 5, (18, 0): 5, (18, 1): 5, (18, 2): 5, (18, 3): 5, (19, 0): 5, (19, 1): 5, (19, 2): 5, (19, 3): 5, (20, 0): 5, (20, 1): 5, (20, 2): 5, (20, 3): 5, (21, 0): 5, (21, 1): 5, (21, 2): 5, (21, 3): 5, (22, 0): 5, (22, 1): 5, (22, 2): 5, (22, 3): 5, (23, 0): 5, (23, 1): 5, (23, 2): 5, (23, 3): 5, (24, 0): 5, (24, 1): 5, (24, 2): 5, (24, 3): 5, (25, 0): 5, (25, 1): 5, (25, 2): 5, (25, 3): 5, (26, 0): 5, (26, 1): 5, (26, 2): 5, (26, 3): 5, (27, 0): 5, (27, 1): 5, (27, 2): 5, (27, 3): 5, (28, 0): 5, (28, 1): 5, (28, 2): 5, (28, 3): 5, (29, 0): 5, (29, 1): 5, (29, 2): 5, (29, 3): 5, (30, 0): 5, (30, 1): 5, (30, 2): 5, (30, 3): 5, (31, 0): 5, (31, 1): 5, (31, 2): 5, (31, 3): 5, (32, 0): 0, (32, 1): 5, (32, 2): 5, (32, 3): 5, (33, 0): 0, (33, 1): 5, (33, 2): 5, (33, 3): 5, (34, 0): 0, (34, 1): 5, (34, 2): 5, (34, 3): 5, (35, 0): 0, (35, 1): 5, (35, 2): 5, (35, 3): 5, (36, 0): 0, (36, 1): 5, (36, 2): 5, (36, 3): 5, (37, 0): 0, (37, 1): 5, (37, 2): 5, (37, 3): 5, (38, 0): 0, (38, 1): 5, (38, 2): 5, (38, 3): 5, (39, 0): 0, (39, 1): 5, (39, 2): 5, (39, 3): 8, (40, 0): 5, (40, 1): 5, (40, 2): 8, (40, 3): 8, (41, 0): 5, (41, 1): 4, (41, 2): 8, (41, 3): 8, (42, 0): 2, (42, 1): 8, (42, 2): 8, (42, 3): 8, (43, 0): 8, (43, 1): 8, (43, 2): 8, (43, 3): 8, (44, 0): 8, (44, 1): 8, (44, 2): 8, (44, 3): 8, (45, 0): 8, (45, 1): 8, (45, 2): 8, (45, 3): 8, (46, 0): 8, (46, 1): 8, (46, 2): 8, (46, 3): 8, (47, 0): 8, (47, 1): 8, (47, 2): 8, (47, 3): 8, (48, 0): 8, (48, 1): 8, (48, 2): 8, (48, 3): 3, (49, 0): 8, (49, 1): 8, (49, 2): 3, (49, 3): 1})\n",
        "current_mover_state =env.reset()\n",
        "for i in range(100):\n",
        "    action_index = policy[current_mover_state]\n",
        "    next_state, reward, done = env.step(current_mover_state, action_index)\n",
        "    print(f\"Step {i+1}: State: {current_mover_state},  action: {env.moves[action_index]}, Next State: {next_state}, Reward: {reward}, resources : {env.resources}\")\n",
        "    current_mover_state = next_state\n",
        "    if done:\n",
        "      break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqqXyGlP-KXk",
        "outputId": "b25003c8-7f9b-4e76-ecbe-8e76dc4c45e6"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: State: (0, 0),  action: Right, Next State: (1, 0), Reward: -1, resources : {(6, 6): 99.7, (1, 5): 99.3}\n",
            "Step 2: State: (1, 0),  action: Up, Next State: (1, 1), Reward: -1, resources : {(6, 6): 99.4, (1, 5): 98.6}\n",
            "Step 3: State: (1, 1),  action: Right, Next State: (2, 1), Reward: -1, resources : {(6, 6): 99.10000000000001, (1, 5): 97.89999999999999}\n",
            "Step 4: State: (2, 1),  action: Up, Next State: (2, 2), Reward: -1, resources : {(6, 6): 98.9, (1, 5): 97.1}\n",
            "Step 5: State: (2, 2),  action: Up, Next State: (2, 3), Reward: -1, resources : {(6, 6): 98.7, (1, 5): 96.3}\n",
            "Step 6: State: (2, 3),  action: Up, Next State: (2, 4), Reward: -1, resources : {(6, 6): 98.5, (1, 5): 95.5}\n",
            "Step 7: State: (2, 4),  action: Up, Next State: (2, 5), Reward: 49, resources : {(6, 6): 98.5, (1, 5): 94.5}\n",
            "Step 8: State: (2, 5),  action: Up, Next State: (2, 6), Reward: -1, resources : {(6, 6): 97.8, (1, 5): 94.2}\n",
            "Step 9: State: (2, 6),  action: Right, Next State: (3, 6), Reward: -1, resources : {(6, 6): 97.1, (1, 5): 93.9}\n",
            "Step 10: State: (3, 6),  action: Right, Next State: (4, 6), Reward: -1, resources : {(6, 6): 96.39999999999999, (1, 5): 93.60000000000001}\n",
            "Step 11: State: (4, 6),  action: Right, Next State: (5, 6), Reward: -1, resources : {(6, 6): 95.69999999999999, (1, 5): 93.30000000000001}\n",
            "Step 12: State: (5, 6),  action: Right, Next State: (6, 6), Reward: 814, resources : {(6, 6): 94.89999999999999, (1, 5): 93.10000000000001}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "\n",
        "moves = ['↑', '↓', '←', '→']\n",
        "\n",
        "data = policy\n",
        "\n",
        "matrix_size = 7  # This should match the highest index values used in your data dictionary\n",
        "\n",
        "confusion_matrix = np.full((matrix_size, matrix_size), '', dtype=object)\n",
        "\n",
        "for (x, y), value in data.items():\n",
        "    if value in range(len(moves)):  # Ensure that the index value is valid\n",
        "        confusion_matrix[y, x] = moves[value]  # Note the inversion of indices for y, x\n",
        "    else:\n",
        "        confusion_matrix[y, x] = 'Invalid'  # Handle invalid data gracefully\n",
        "\n",
        "confusion_matrix = np.flipud(confusion_matrix)\n",
        "\n",
        "labels = [str(i) for i in range(matrix_size)]\n",
        "confusion_matrix[0,6] = 'Done'\n",
        "# Generate a textual representation of the matrix using tabulate\n",
        "table = tabulate(confusion_matrix, headers=labels, tablefmt=\"grid\", showindex=labels[::-1])\n",
        "\n",
        "print(table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hm3DJmT-NM1",
        "outputId": "bb6e00e3-86cd-4892-fa02-642e14624ad4"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+-----+-----+-----+-----+-----+------+\n",
            "|    | 0   | 1   | 2   | 3   | 4   | 5   | 6    |\n",
            "+====+=====+=====+=====+=====+=====+=====+======+\n",
            "|  6 | →   | ↓   | →   | →   | →   | →   | Done |\n",
            "+----+-----+-----+-----+-----+-----+-----+------+\n",
            "|  5 | →   | ↓   | ↑   | →   | →   | ↑   | ←    |\n",
            "+----+-----+-----+-----+-----+-----+-----+------+\n",
            "|  4 | →   | →   | ↑   | ←   | ←   | ↑   | ↑    |\n",
            "+----+-----+-----+-----+-----+-----+-----+------+\n",
            "|  3 | →   | ↓   | ↑   | ←   | ↓   | ↑   | ↑    |\n",
            "+----+-----+-----+-----+-----+-----+-----+------+\n",
            "|  2 | ↓   | ↓   | ↑   | ←   | ←   | ←   | ←    |\n",
            "+----+-----+-----+-----+-----+-----+-----+------+\n",
            "|  1 | ↓   | →   | ↑   | ←   | ↑   | ←   | ↑    |\n",
            "+----+-----+-----+-----+-----+-----+-----+------+\n",
            "|  0 | →   | ↑   | ↑   | ←   | ←   | ←   | ↑    |\n",
            "+----+-----+-----+-----+-----+-----+-----+------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhOBDTK5DsYK",
        "outputId": "986a4eff-8480-4060-ba05-36ae7bb7e2db"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 0): 0,\n",
              " (0, 1): 0,\n",
              " (0, 2): 0,\n",
              " (0, 3): 0,\n",
              " (0, 4): 0,\n",
              " (0, 5): 3,\n",
              " (0, 6): 1,\n",
              " (1, 0): 2,\n",
              " (1, 1): 1,\n",
              " (1, 2): 2,\n",
              " (1, 3): 2,\n",
              " (1, 4): 0,\n",
              " (1, 5): 3,\n",
              " (1, 6): 1,\n",
              " (2, 0): 2,\n",
              " (2, 1): 0,\n",
              " (2, 2): 0,\n",
              " (2, 3): 2,\n",
              " (2, 4): 0,\n",
              " (2, 5): 3,\n",
              " (2, 6): 2,\n",
              " (3, 0): 0,\n",
              " (3, 1): 0,\n",
              " (3, 2): 0,\n",
              " (3, 3): 0,\n",
              " (3, 4): 0,\n",
              " (3, 5): 0,\n",
              " (3, 6): 3,\n",
              " (4, 0): 2,\n",
              " (4, 1): 0,\n",
              " (4, 2): 0,\n",
              " (4, 3): 2,\n",
              " (4, 4): 2,\n",
              " (4, 5): 0,\n",
              " (4, 6): 3,\n",
              " (5, 0): 2,\n",
              " (5, 1): 1,\n",
              " (5, 2): 3,\n",
              " (5, 3): 3,\n",
              " (5, 4): 0,\n",
              " (5, 5): 0,\n",
              " (5, 6): 3,\n",
              " (6, 0): 1,\n",
              " (6, 1): 1,\n",
              " (6, 2): 3,\n",
              " (6, 3): 0,\n",
              " (6, 4): 0,\n",
              " (6, 5): 2,\n",
              " (6, 6): 0}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KXxvx627EqBC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}